{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228a213c",
   "metadata": {},
   "source": [
    "# Deep Residual Learning for Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352f8e3",
   "metadata": {},
   "source": [
    "[Deep residual Nets](https://arxiv.org/pdf/1512.03385.pdf) is one of most influential papers of modern Deep Learning. We'll cover import concepts and intuition behind the idea. The reader can refer to the original paper for detailed information on training procedure and improvements on different benchmark tasks and datasets. \n",
    "\n",
    "<i><u>Overview of the notebook </u></i>\n",
    "\n",
    "1. Background Problem Statement\n",
    "2. Residual learning\n",
    "3. How Residual learning counters vanishing gradient\n",
    "4. Residual Network Architecture\n",
    "5. Results of Residual Learning\n",
    "6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ddab8",
   "metadata": {},
   "source": [
    "## 1. Background Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62080405",
   "metadata": {},
   "source": [
    "- Deep NNs are able to create multiple levels of rich features (low/mid/high-level) from image data\n",
    "- `The richness of features is directly on the level / depth` of layer, meaning stacking more layers would result in more detailed features, leading to greater accuracy.\n",
    "- SOTA models of that time used *16-30 layers*\n",
    "- *Authors question : Is learning better networks as easy as stacking more layers?*\n",
    "- One bottleneck till that point was Vanishing gradient problem. `Batch Normalization helped in solving the problem, which resulted in convergence of deeper netrworks`\n",
    "- But `Deeper nets faced a degradation problem when converging. (i.e) deeper layers learning got saturated`\n",
    "- This was not overfitting as the training error was also high as you can see in the image below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb53a7e",
   "metadata": {},
   "source": [
    "![Deeper nets Training difficulty](images/problemStatement.png)\n",
    "\n",
    "<center>\n",
    "    Reference : <a href=\"https://arxiv.org/pdf/1512.03385.pdf\">Deep residual Nets</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c39a18",
   "metadata": {},
   "source": [
    "#### Simple Experiment\n",
    "\n",
    "- Consider two networks - a shallow and a deeper one where the deeper one contained the same layers as the shallow one but stacked with extra layers\n",
    "- In ideal scenario, `we'd expect the deeper network to produce same, if not lesser error than shallower one by mapping deeper layers to identity function`. Expected solution will be something like :\n",
    "\n",
    "![Constructed Deeper net solution](images/solutionConstruction.png)\n",
    "\n",
    "- But solutions found by solvers were worse than the shallow networks suggesting `deeper networks were harder to train`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cb2ee",
   "metadata": {},
   "source": [
    "## 2. Residual learning\n",
    "\n",
    "- Authors propose a `residual learning` approach where the `networks try to learn the residual of the target function` than the original function itself. (i.e) Assuming *x* to be the network input and **H(x)** to be the original function to learn, we reformulate the function as **F(x) = H(x) - x**. It can be described as follows : \n",
    "\n",
    "![Residual block](images/residualBlock.png)\n",
    "\n",
    "The layers are made to learn the non-linear mapping ***F(x)*** while an `extra skip-connection or shortcut connection` helps us get back to the original function *H(x)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae3a03",
   "metadata": {},
   "source": [
    "### Hypothesis for residual learning\n",
    "- Ideally as universal function approximators, the deeper layers must have been able to learn the identity mapping. But not being able to do so suggests there exists a `learning degradation` problem. So we try to precondition them by adding a reference identity mapping\n",
    "- Authors hypothesize `ease of learning residual function than an unreference non-linear function`\n",
    "- ***Authors propose that if identity mappings were optimal, the solvers might just drive the weights of deeper layers to zero***\n",
    "- ***Its assumed that identity mappings are optimal, which unlikely to be true in all cases*** but still the layer responses support the above assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65efcd2",
   "metadata": {},
   "source": [
    "### Identity mapping by shortcuts\n",
    "\n",
    "Equation for the residual block is as follows : \n",
    "\n",
    "![residualBlockEquation](images/residualBlockEquation.png)\n",
    "\n",
    "\n",
    "where **F(x,W)** *is called a building block which tries to learn residual function*, essentially a series of weight layers followed by non-linear activation function. **W_s x** *indicates shortcut connection* where the sizes dont match and we add `projection matrices`. \n",
    "- The size can be matched by *identity mapping and zero padding* or *projection matrices (learned parameters)*  \n",
    "- If the feature map sizes are same, then we can use **identity shortcut (element-wise addition) which is parameter-free and has negligible additional time complexity**. \n",
    "- The authors point out that this is crucial in comparing plain vs residual networks performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100814e",
   "metadata": {},
   "source": [
    "## 3. How Residual learning affects Gradient flow\n",
    "\n",
    "Consider two sample networks as follows:\n",
    "\n",
    "<table>\n",
    "<tr><h4>demo_Plain_Network  (without skip-connection)</h4></tr>   \n",
    "\n",
    "<tr>\n",
    "<td> <img src=\"images/demoPlainNet.png\" width=\"800\" height=\"300\"> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3cdd2",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><h4>demo_Residual_Network (with skip-connection)</h4></tr>   \n",
    "<tr>\n",
    "<td> <img src=\"images/demoResidualNet.png\" style=\"width:150%\"> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9b234",
   "metadata": {},
   "source": [
    "- Both demoPlainNet and demoResidualNet are identical except for the additional skip-connection pathways\n",
    "- A sample batch of input  from [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset is fed through both the networks\n",
    "- The magnitude of gradients is observed across different layers of both networks\n",
    "- The ratio of magnitude of the gradients for the conv1 block of 1st Layer of Residual Net w.r.t Plain Net looks like below :\n",
    "<img src=\"images/Layer1_0_conv1_weight_gradient.png\" style=\"width:60%\">\n",
    "- As you can see, many filters in residual blocks have **gradients > 1**\n",
    "- ***Rationale : The skip-connection pathway is not scaled by the magnitude of the intermediate layers and thus allows gradients to flow much earlier in the network and with much greater effect***\n",
    "- Following images shows gradient flow between a sample plain-network and same network with identity skip-connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40435ea4",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td> <img src=\"images/normalConnectionMath.jpg\" style=\"width:90%\"> </td>\n",
    "<td> <img src=\"images/skipConnection_normalConnection_comparison.jpg\"  style=\"width:90%\"> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a761b5",
   "metadata": {},
   "source": [
    "- The code for the above visualization comparing Plain vs Skip-connection network on FashionMNIST dataset can be found at this [kaggle notebook](https://www.kaggle.com/suryajrrafl/plain-vs-residualnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857dc5b4",
   "metadata": {},
   "source": [
    "## 4. Residual Network Architecture\n",
    "\n",
    "- The architecture itself is inspired from the *VGGNet* where series of **Conv -> Batch Norm -> ReLU Activation** blocks are used. The authors quote the following principles were followed when designing the architecture\n",
    "    1. For the same output feature map size, the layers have the same number of filters\n",
    "    2. If the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer (*downsampling is done by using conv layer with stride of 2*)\n",
    "    \n",
    "    \n",
    "- The feature map size reduces while the number of channels increases as we go deeper into the network\n",
    "- Finally we do averge pooling followed by a fully connected layer to match number of output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb2ee5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h4> Comparison between 34-layer Plain and Residual networks </h4>\n",
    "</center>\n",
    "\n",
    "\n",
    "![plain_vs_resnet](images/plain_vs_resnet.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456e8b5",
   "metadata": {},
   "source": [
    "- The solid shortcut connections indicate identity mappings\n",
    "- The dotted shortcut connections indicate projection matrix approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46685d48",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h4> Resnet Family Table</h4>\n",
    "</center> \n",
    "\n",
    "\n",
    "![resnetFamilyTable](images/resnetFamilyTable.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f64072",
   "metadata": {},
   "source": [
    "### Identity vs. Projection Shortcuts\n",
    "- The parameter-free, identity shortcuts help with training. The Projection shortcuts mentioned earlier can be achieved in 3 ways : \n",
    "    - Option A : zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter free\n",
    "    - Option B : projection shortcuts are used for increasing dimensions, and other shortcuts are identity\n",
    "    - Option C : All shortcuts are projections.\n",
    "\n",
    "- `Option A < Option B < Option C`. B is better than A because A is just zero padding and no residual learning. C is marginally better than B due to extra parameters, but `Option B is preferred due to better generalization and << time complexity than Option C`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613814d7",
   "metadata": {},
   "source": [
    "### Basic  Blocks\n",
    "- Basic block containing 2 layers of conv-batchnorm-relu modules\n",
    "- Used in resnet18 and resnet34 architectures\n",
    "\n",
    "![BasicBlock](images/basicBlock.png)\n",
    "\n",
    "\n",
    "### BottleNeck  Blocks\n",
    "- Bottleneck block containing 3 layers of conv-batchnorm-relu modules\n",
    "- The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions.\n",
    "- Used in resnet50, resnet101 and resnet152 architectures\n",
    "\n",
    "![BottleNeckBlock](images/bottleNeckBlock.png)\n",
    "\n",
    "\n",
    "- **NOTE** : The parameter-free identity shortcuts are particularly important for the bottleneck architectures. If the identity shortcut is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more efficient models for the bottleneck designs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a255353",
   "metadata": {},
   "source": [
    "## 5. Results of Residual Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd67d8",
   "metadata": {},
   "source": [
    "The residual networks showed significant improvement over their plain counterparts and more importantly provided way to train very deep architectures\n",
    "\n",
    "<center>\n",
    "    <h5> Plain vs Residual Networks top1% Test Error on Imagenet</h5>\n",
    "</center> \n",
    "\n",
    "![plain_vs_resnet_results_table](images/plain_vs_resnet_results_table.png)\n",
    "\n",
    "\n",
    "<u> <i> Observations </i></u>\n",
    "- Residual networks are better than Plain Networks for both 18 and 34 layer architecture\n",
    "- 34-layer Plain network has higher error than 18-layer Plain Network\n",
    "- 34-layer Residual network has lesser error than 18-layer Residual Network\n",
    "- 18-layer Residual network has same error as the 18-layer Plain network\n",
    "\n",
    "<center>\n",
    "    <h5> Plain vs Residual Networks Training Validation curves </h5>\n",
    "</center> \n",
    "\n",
    "![plain_vs_resnet_Training curves](images/plain_vs_resnet_results_curves_plot.png)\n",
    "\n",
    "\n",
    "### Inference\n",
    "- One can see that Training error for 34-layer Plain Network is more than 18-layer network, proving the degradation problem.\n",
    "- The trend is reversed for the Residual networks where the 34-layer networks has lesser training and validation error than the 18-layer network\n",
    "- The 18-layer Residual network, though similar in accuracy to the 18-layer Plain network ***converges faster***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b04c4d",
   "metadata": {},
   "source": [
    "### Analysis of Layer Responses\n",
    "\n",
    "![layerResponses](images/layerResponses.png)\n",
    "\n",
    "- For ResNets, this analysis reveals the response strength of the residual functions.\n",
    "- It is evident from above image that ResNets have generally smaller responses than their plain counterparts. \n",
    "- These results support our basic motivation that the residual functions might be generally closer to zero than the non-residual functions.\n",
    "- We also notice that the deeper ResNet has smaller magnitudes of responses, as evidenced by the comparisons among ResNet-20, 56, and 110. \n",
    "- When there are more layers, an individual layer of ResNets tends to modify the signal less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeabc52",
   "metadata": {},
   "source": [
    "\n",
    "## 6. References\n",
    "\n",
    "1. [Resnet function classes](https://d2l.ai/chapter_convolutional-modern/resnet.html)\n",
    "2. [visio blog post](https://viso.ai/deep-learning/resnet-residual-neural-network/)\n",
    "3. [cv tricks blog post](https://cv-tricks.com/keras/understand-implement-resnets/)\n",
    "4. [Great learning blog post](https://www.mygreatlearning.com/blog/resnet/)\n",
    "5. [Understanding and visualizing resnsets](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda(dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
